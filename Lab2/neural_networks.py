# -*- coding: utf-8 -*-
"""
Created on Sat Nov 23 18:18:00 2019

@author: Ana
"""

"""
Neural Networks: 
Train a Multi-Layer perceptron using the cross-entropy loss with l-2 regularization (weight decay penalty). 
In other words, the activation function equals the logistic function. 
Plot curves of the training and validation error as a function of the penalty strength alpha. 
How do the curves behave? Explain why.
Advice: use a logaritmic range for hyper-parameter alpha. 
Experiment with different sizes of the training/validation sets and different model parameters (network layers).
"""

# https://gluon.mxnet.io/chapter03_deep-neural-networks/mlp-scratch.html